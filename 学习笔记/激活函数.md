### 什么是激活函数
网络中每个神经元都以上一层神经元的输出值作为输入。在多层网络中，将输出值作为输入值之前，需要先进行一次函数转换，这个函数就是激活函数。

### 为什么需要激活函数
不适用激活函数（相当于激活函数为f = x），那么每一层节点的输入都是上层输出的线性函数。有人验证无论神经网络有多少层，输出如果是输入的线性组合，那么这与没有隐藏层的效果相当，
网络的逼近能力也相当有限。因此我们引入非线性函数作为激活函数，提升神经网络表达能力（不再是线性组合，而是可以逼近任何函数）。

### 有哪些激活函数？各自特点？
#### sigmoid
$$f(z)=\frac{1}{1+e^{-z}}$$
其图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/sigmoid.png)

它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。其导数图像为
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/derivativesigmoid.png)

**缺点1**：发生梯度消失的概率大。在反向传播算法中，计算得到的$\delta ^{i,l}$是以激活函数的导数为系数的。而如果所有隐层都使用sigmoid函数，那么除了第一层的输入和最后一层的输出，其他所有的输入和输出都是0-1，而如果是0-1之间，sigmoid的导数图像大致取值为0.2-0.25。这会导致$\delta ^{i,l}$不断减小，趋近于0，梯度消失。

**缺点2**：sigmoid不是zero-centered。这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。结果就是如x>0，$f = wx+b$对x求局部梯度则全都是正。**这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，使得收敛缓慢。**

**缺点3**：包含幂运算，费时间。

#### tanh (读作Hyperbolic Tangent)
$$ tanh(x) =\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $$
其图像和导数图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/tanh.png)

解决了sigmoid不是zero-centered问题，其他问题依旧存在。

#### ReLU
$$ ReLU = max(0,x)$$
其图像和导数图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/relu.png)

ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：

1. 解决了gradient vanishing问题 (在正区间)
2. 计算速度非常快，只需要判断输入是否大于0
3. 收敛速度远快于sigmoid和tanh

ReLU也有需要特别注意的问题：

1. 不是zero-centered
2. Dead ReLU Problem，某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是**可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。**


#### Leaky ReLU (PReLU)
$$ f(x)=max(\alpha x, x) $$
其函数图像($\alpha$很小)和导数图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/prelu.png)

人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为$\alpha x$而非0，通常$\alpha$=0.01。另外一种直观的想法是基于参数的方法，即ParametricReLU:f(x)=max($\alpha$x,x)，其中$\alpha$可由方向传播算法学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。

#### ELU (Exponential Linear units)
$$ f(x) = x /quad if /quad x>0 /quad else /quad \alpha (e^x-1) $$
其函数图像和导数图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/elu.png)
#### MaxOut

### 如何选择？
这个问题目前没有确定的方法，凭一些经验吧。

1）深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。

2）如果使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.

3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout.
