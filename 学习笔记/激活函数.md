### 什么是激活函数
网络中每个神经元都以上一层神经元的输出值作为输入。在多层网络中，将输出值作为输入值之前，需要先进行一次函数转换，这个函数就是激活函数。

### 为什么需要激活函数
不适用激活函数（相当于激活函数为f = x），那么每一层节点的输入都是上层输出的线性函数。有人验证无论神经网络有多少层，输出如果是输入的线性组合，那么这与没有隐藏层的效果相当，
网络的逼近能力也相当有限。因此我们引入非线性函数作为激活函数，提升神经网络表达能力（不再是线性组合，而是可以逼近任何函数）。

### 有哪些激活函数？各自特点？
#### sigmoid
$$f(z)=\frac{1}{1+e^{-z}}$$
其图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/sigmoid.png)

它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。其导数图像为
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/derivativesigmoid.png)

**缺点1**：发生梯度消失的概率大。在反向传播算法中，计算得到的$\delta ^{i,l}$是以激活函数的导数为系数的。而如果所有隐层都使用sigmoid函数，那么除了第一层的输入和最后一层的输出，其他所有的输入和输出都是0-1，而如果是0-1之间，sigmoid的导数图像大致取值为0.2-0.25。这会导致$\delta ^{i,l}$不断减小，趋近于0，梯度消失。

**缺点2**：sigmoid不是zero-centered。这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。结果就是如x>0，$f = wx+b$对x求局部梯度则全都是正。**这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，使得收敛缓慢。**

**缺点3**：包含幂运算，费时间。

#### tanh (读作Hyperbolic Tangent)
$$ tanh(x) =\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $$
其图像和导数图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/tanh.png)

解决了sigmoid不是zero-centered问题，其他问题依旧存在。

#### ReLU
$$ ReLU = max(0,x)$$
其图像和导数图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/relu.png)

ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：

1. 解决了gradient vanishing问题 (在正区间)
2. 计算速度非常快，只需要判断输入是否大于0
3. 收敛速度远快于sigmoid和tanh

ReLU也有需要特别注意的问题：

1. 不是zero-centered
2. Dead ReLU Problem，某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是**可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。**


#### Leaky ReLU (PReLU)
$$ f(x)=max(\alpha x, x) $$
其函数图像($\alpha$很小)和导数图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/prelu.png)

人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为$\alpha x$而非0，通常$\alpha$=0.01。另外一种直观的想法是基于参数的方法，即ParametricReLU:f(x)=max($\alpha$x,x)，其中$\alpha$可由方向传播算法学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。

#### ELU (Exponential Linear units)
$$ f(x) = x(x>0)  $$
$$ f(x) = \alpha (e^x-1) (otherwise) $$
其函数图像和导数图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/elu.png)

ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：

1. 不会有Dead ReLU问题

2. 输出的均值接近0，zero-centered

它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。
#### MaxOut
Maxout是深度学习网络中的一层网络，就像池化层、卷积层一样等，我们可以把maxout 看成是网络的激活函数层。我们假设网络某一层的输入特征向量为：X=（x1,x2,……xd），也就是我们输入是d个神经元。Maxout隐藏层每个神经元的计算公式如下：
$$ h_i(x) = max_{j\in [i,k]} z_{ij} $$
k就是maxout层所需要的参数，人为设定。公式中z：
$$ z_{ij} = w_{ij}x+b_{ij} $$
权重w是一个（d,m,k）的三维矩阵，b是一个（m,k）的二维矩阵，这些是我们需要学习的。

例子：假设我们网络第i层有2个神经元x1、x2，第i+1层的神经元个数为1个

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/maxout1.png)

我们要计算第i+1层，那个神经元的激活值的时候：
$$out = f(z) = f(Wx +b)$$
其中f就是激活函数。而如果我们使用maxout，设置k=5，那么

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/maxout2.png)

相当于在每个输出神经元前面又多了一层。这一层有5个神经元，此时maxout网络的输出计算公式为：
$$z_1=w_1 * x+b_1$$
$$z_2=w_2 * x+b_2$$
$$z_3=w_3 * x+b_3$$
$$z_4=w_4 * x+b_4$$
$$z_5=w_5 * x+b_5$$
$$out = max(z_1,z_2,z_3,z_4,z_5)$$
所以这就是为什么采用maxout的时候，参数个数成k倍增加的原因。本来我们只需要一组参数就够了，采用maxout后，就需要有k组参数。

**maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。**

**Maxout具有ReLU的所有优点，线性、不饱和性。**

**同时没有ReLU的一些缺点。如：神经元的死亡。**

### 如何选择？
这个问题目前没有确定的方法，凭一些经验吧。

1）深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。

2）如果使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.

3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout.
