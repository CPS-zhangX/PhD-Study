### 什么是激活函数
网络中每个神经元都以上一层神经元的输出值作为输入。在多层网络中，将输出值作为输入值之前，需要先进行一次函数转换，这个函数就是激活函数。

### 为什么需要激活函数
不适用激活函数（相当于激活函数为f = x），那么每一层节点的输入都是上层输出的线性函数。有人验证无论神经网络有多少层，输出如果是输入的线性组合，那么这与没有隐藏层的效果相当，
网络的逼近能力也相当有限。因此我们引入非线性函数作为激活函数，提升神经网络表达能力（不再是线性组合，而是可以逼近任何函数）。

### 有哪些激活函数？各自特点？
#### sigmoid
$$f(z)=\frac{1}{1+e^{-z}}$$
其图像为：
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/sigmoid.png)
#### tanh

#### ReLU

#### Leaky ReLU (PReLU)

#### ELU (Exponential Linear units)

#### MaxOut

### 如何选择？
这个问题目前没有确定的方法，凭一些经验吧。

1）深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。

2）如果使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.

3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout.
