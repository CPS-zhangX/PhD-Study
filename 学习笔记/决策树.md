### 决策树
一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果。常用的生成算法包括ID3, C4.5, C5.0，CART（**这种一般最优**）。需要分类结果已知的数据（监督学习）：

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/decisiontree.jpg)

然后用这一组附带分类结果的样本可以训练出多种多样的决策树，这里为了简化过程，我们假设决策树为二叉树:

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/decisiontree1.jpg)

通过学习上表的数据，可以设置A，B，C，D，E的具体值，而A，B，C，D，E则称为阈值。当然也会生成与上图完全不同的树形。

所以决策树的生成主要分以下两步，**这两步通常通过学习已经知道分类结果的样本来实现**。

1.节点的分裂

2 阈值的确定（合适的阈值可以减少training error）

#### ID3

由增熵（Entropy）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。

Entropy = $ -\sum_{i=1}^n [P(x_i)* log_x(P(x_i))] $

#### C4.5
其中C5.0是4.5的商业版，很多细节并不公布，但是核心思想和4.5是类似的。

#### CART 分类回归树

- [1] [决策树：通俗易懂的介绍](https://zhuanlan.zhihu.com/p/30059442)
