### 决策树
一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果。常用的生成算法包括ID3, C4.5, C5.0，CART（**这种一般最优**）。需要分类结果已知的数据（监督学习）：

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/decisiontree.jpg)

然后用这一组附带分类结果的样本可以训练出多种多样的决策树，这里为了简化过程，我们假设决策树为二叉树:

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/decisiontree1.jpg)

通过学习上表的数据，可以设置A，B，C，D，E的具体值，而A，B，C，D，E则称为阈值。当然也会生成与上图完全不同的树形。

所以决策树的生成主要分以下两步，**这两步通常通过学习已经知道分类结果的样本来实现**。

1.节点的分裂

2 阈值的确定（合适的阈值可以减少training error）

#### ID3 (昆兰)

由增熵（Entropy，记作H）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。

H = $ -\sum_{i=1}^n [P(x_i)* log_2(P(x_i))] $，其中$P(x_i)$是$x_i$出现的概率。比如我们有15个样本D，输出为0或者1。其中有9个输出为1， 6个输出为0。 样本中有个特征A，取值为A1，A2和A3。在取值为A1的样本的输出中，有3个输出为1， 2个输出为0，取值为A2的样本输出中,2个输出为1,3个输出为0， 在取值为A3的样本中，4个输出为1，1个输出为0。

样本D的熵为：$H(D) = -(\frac{9}{15}log_2(\frac{9}{15}) + \frac{6}{15}log_2(\frac{6}{15})) = 0.971$

样本D在特征下的条件熵为：$H(D|A) = \frac{5}{15}E(D1)+\frac{5}{15}E(D2)+\frac{5}{15}E(D3) = -\frac{5}{15}(\frac{3}{5}log_2\frac{3}{5} + \frac{2}{5}log_2\frac{2}{5}) - \frac{5}{15}(\frac{2}{5}log_2\frac{2}{5} + \frac{3}{5}log_2\frac{3}{5}) - \frac{5}{15}(\frac{4}{5}log_2\frac{4}{5} + \frac{1}{5}log_2\frac{1}{5}) = 0.888$

其中，熵度量事物的不确定性，条件熵表示我们的D在知道Y之后剩下的不确定性。

互信息I(D,A)，表示D在知道了A之后不确定性减少的程度。$ I(D,A) = H(D) - H(D|A) = 0.083 $，该值越大越应该尽早选择。那么ID3算法大致可以表述为：

**输入：**m个样本，样本输出集合为D，每个样本有n个离散特征，特征集合即为A。

**输出：**决策树T

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/DT.png)

该算法的问题在于：

1. 没有考虑连续特征，比如长度，密度都是连续值

2. ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。(**用信息增益作为标准容易偏向于取值较多的特征**)

3. 没有考虑缺失值的情况

4. 越细小的分割越精确，但这会overfitting

#### C4.5(昆兰)

主要针对上述的四个问题进行解决。

**solution for 1**：连续的特征离散化。比如m个样本的连续特征A有m个，从小到大为$a_1,a_2,\ldots, a_m$，则C4.5取相邻两样本值的平均数，一共取得m-1个划分点。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$为类别1，大于为类别2。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程（即不用执行A = A-{Ag}）。

**solution for 2**：信息增益作为标准容易偏向于取值较多的特征的问题。我们引入一个信息增益比的变量IR(X,Y)，它是信息增益和特征熵的比值。

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/DT1.png)

**solution for 3**：如何处理缺失？主要需要解决的是两个问题，一是在特征值缺失的情况下进行特征的划分（**如何计算特征的信息增益率**）？二是知道了如何划分，对于在该属性上缺失特征的样本的处理（**把缺失特征的样本划分到哪个节点？**）。

1. 对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。

2. 将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。

**solution for 4**：剪枝，详见后面。

其中C5.0是4.5的商业版，很多细节并不公布，但是核心思想和4.5是类似的。

该算法也是有优化空间的：

1. 决策树算法非常容易过拟合，还是需要进行剪枝（预剪枝，后剪枝+cross validation）

2. 生成多叉树，效率没有二叉树快

3. 不能用于回归问题

4. 有对数，排序等耗时运算

#### CART 分类回归树

如果不考虑集成学习话，在普通的决策树算法里，CART算法算是比较优的算法了。**scikit-learn的决策树使用的也是CART算法**。

CART只能将一个父节点分为两个子节点，如何分裂通过GINI指数（基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好）来确定。比如：

a. 比如出勤率大于70%这个条件将训练数据分成两组：大于70%里面有两类：【好学生】和【不是好学生】，而小于等于70%里也有两类：【好学生】和【不是好学生】。

b. 如果用分数小于70分来分：则小于70分只有【不是好学生】一类，而大于等于70分有【好学生】和【不是好学生】两类。

**发现b的凌乱程度比a要小，即GINI指数b比a小，所以选择b的方案**。

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/DT2.png)

**对于连续值的处理**：和C4.5一样，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。

**对于离散值的处理**：不停的二分。如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，如果是ID3或者C4.5会建立三叉。但CART分类树会考虑把A分成{A1}和{A2,A3}, {A2}和{A1,A3}, {A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3},然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。

#### CART分类树建立算法的具体流程（不包含剪枝）
输入：训练集D，基尼系数的阈值，样本个数阈值。

输出：决策树T。

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/DT3.png)

对于生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别。

#### CART回归树建立算法（不含剪枝）
与分类树大致相同，区别在于：

1. 连续值的处理不同

2. 决策树建立后做预测的方式不同

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/DT4.png)

#### 剪枝

CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数，算法基本完全一样。

**为什么剪枝？**由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。

采用**后剪枝**的方法，具体而言：第一步是从原始决策树生成各种剪枝效果的决策树，第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/DT5.png)

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/DT6.png)

**如果样本发生一点点变化，那么树的结构会剧烈变动，这可以通过随机森林的方法解决**

### 总结
![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/DT7.png)

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/DT8.png)

- [1] [决策树：通俗易懂的介绍](https://zhuanlan.zhihu.com/p/30059442)

- [2] [决策树算法原理 上](https://www.cnblogs.com/pinard/p/6050306.html)

- [3] [决策树算法原理 下](https://www.cnblogs.com/pinard/p/6053344.html)

- [4] [【机器学习】决策树（上）——ID3、C4.5、CART](https://zhuanlan.zhihu.com/p/85731206)

