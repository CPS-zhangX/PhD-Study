### 决策树
一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果。常用的生成算法包括ID3, C4.5, C5.0，CART（**这种一般最优**）。需要分类结果已知的数据（监督学习）：

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/decisiontree.jpg)

然后用这一组附带分类结果的样本可以训练出多种多样的决策树，这里为了简化过程，我们假设决策树为二叉树:

![image](https://raw.githubusercontent.com/CPS-zhangX/PhD-Study/master/images/decisiontree1.jpg)

通过学习上表的数据，可以设置A，B，C，D，E的具体值，而A，B，C，D，E则称为阈值。当然也会生成与上图完全不同的树形。

所以决策树的生成主要分以下两步，**这两步通常通过学习已经知道分类结果的样本来实现**。

1.节点的分裂

2 阈值的确定（合适的阈值可以减少training error）

#### ID3

由增熵（Entropy，记作H）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。

H = $ -\sum_{i=1}^n [P(x_i)* log_2(P(x_i))] $，其中$P(x_i)$是$x_i$出现的概率。比如我们有15个样本D，输出为0或者1。其中有9个输出为1， 6个输出为0。 样本中有个特征A，取值为A1，A2和A3。在取值为A1的样本的输出中，有3个输出为1， 2个输出为0，取值为A2的样本输出中,2个输出为1,3个输出为0， 在取值为A3的样本中，4个输出为1，1个输出为0。

样本D的熵为：$H(D) = -(\frac{9}{15}log_2(\frac{9}{15}) + \frac{6}{15}log_2(\frac{6}{15})) = 0.971$

样本D在特征下的条件熵为：$H(D|A) = \frac{5}{15}E(D1)+\frac{5}{15}E(D2)+\frac{5}{15}E(D3) = -\frac{5}{15}(\frac{3}{5}log_2\frac{3}{5} + \frac{2}{5}log_2\frac{2}{5}) - \frac{5}{15}(\frac{2}{5}log_2\frac{2}{5} + \frac{3}{5}log_2\frac{3}{5}) - \frac{5}{15}(\frac{4}{5}log_2\frac{4}{5} + \frac{1}{5}log_2\frac{1}{5}) = 0.888$

其中，熵度量事物的不确定性，条件熵表示我们的D在知道Y之后剩下的不确定性。

互信息I(D,A)，表示D在知道了A之后不确定性减少的程度。$ I(D,A) = H(D) - H(D|A) = 0.083 $。那么ID3算法大致可以表述为：

1. 
2. 

#### C4.5
其中C5.0是4.5的商业版，很多细节并不公布，但是核心思想和4.5是类似的。

#### CART 分类回归树

- [1] [决策树：通俗易懂的介绍](https://zhuanlan.zhihu.com/p/30059442)
